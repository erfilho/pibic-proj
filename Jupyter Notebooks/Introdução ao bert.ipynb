{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ('Hello, how are you? I am Romeo.\\n'\n",
    "       'Hello, Romeo My name is Juliet. Nice to meet you.\\n'\n",
    "       'Nice meet you too. How are you today?\\n'\n",
    "       'Great. My baseball team won the competition.\\n'\n",
    "       'Oh Congratulations, Juliet\\n'\n",
    "       'Thanks you Romeo')\n",
    "\n",
    "sentences = re.sub(\"[.,!?\\\\-]\", '', text.lower()).split('\\n')  # filter '.', ',', '?', '!'\n",
    "\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "\n",
    "word_dict = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "\n",
    "for i, w in enumerate(word_list):\n",
    "   word_dict[w] = i + 4\n",
    "   number_dict = {i: w for i, w in enumerate(word_dict)}\n",
    "   vocab_size = len(word_dict)\n",
    "\n",
    "def make_batch():\n",
    "   batch = []\n",
    "   positive = negative = 0\n",
    "   while positive != batch_size/2 or negative != batch_size/2:\n",
    "       tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences)) \n",
    "\n",
    "       tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
    "\n",
    "       input_ids = [word_dict['[CLS]']] + tokens_a + [word_dict['[SEP]']] + tokens_b + [word_dict['[SEP]']]\n",
    "\n",
    "\n",
    "       segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
    "\n",
    "       # MASK LM\n",
    "       n_pred =  min(max_pred, max(1, int(round(len(input_ids) * 0.15)))) # 15 % of tokens in one sentence\n",
    "       cand_maked_pos = [i for i, token in enumerate(input_ids)\n",
    "                         if token != word_dict['[CLS]'] and token != word_dict['[SEP]']]\n",
    "       shuffle(cand_maked_pos)\n",
    "       masked_tokens, masked_pos = [], []\n",
    "       for pos in cand_maked_pos[:n_pred]:\n",
    "           masked_pos.append(pos)\n",
    "           masked_tokens.append(input_ids[pos])\n",
    "           if random() < 0.8:  # 80%\n",
    "               input_ids[pos] = word_dict['[MASK]'] # make mask\n",
    "           elif random() < 0.5:  # 10%\n",
    "               index = randint(0, vocab_size - 1) # random index in vocabulary\n",
    "               input_ids[pos] = word_dict[number_dict[index]] # replace\n",
    "\n",
    "       # Zero Paddings\n",
    "       n_pad = maxlen - len(input_ids)\n",
    "       input_ids.extend([0] * n_pad)\n",
    "       segment_ids.extend([0] * n_pad)\n",
    "\n",
    "       # Zero Padding (100% - 15%) tokens\n",
    "       if max_pred > n_pred:\n",
    "           n_pad = max_pred - n_pred\n",
    "           masked_tokens.extend([0] * n_pad)\n",
    "           masked_pos.extend([0] * n_pad)\n",
    "\n",
    "       if tokens_a_index + 1 == tokens_b_index and positive < batch_size/2:\n",
    "           batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
    "           positive += 1\n",
    "       elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
    "           batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
    "           negative += 1\n",
    "   return batch"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
